---
title: "Untitled"
format: html
editor: visual
editor_options: 
  chunk_output_type: console
---


## Packages

```{r packages}

library(tidyverse)
library(survival)
library(logistf)
library(glmmTMB)
library(DHARMa)
library(boot)
library(broom.mixed)
library(broom)
library(SuppDists)
library(flexsurv)


library(coda)
library(performance)

library(brms)
library(bayestestR)
library(bayesplot)

```

## Directory

```{r}

Dir_RatopRda <- "C:/Users/laura/Dropbox/PhD_20150515/Research projects/DMR_Ratop/Rda" 

Dir_DCRda <- "C:/Users/laura/Dropbox/PhD_20150515/Research projects/DMR_DC/Analyses/Data/Rda"

```



## Data

```{r}
setwd(Dir_DCRda)
load("DC_Info_F.rda")

```



## Effect of treatment on delay til first parturition

```{r Data}

FirstPart_Data <- DC_Info_F %>% 
  group_by(ExperimentalGroup) %>% 
  slice_min(FirstDCParturitionDate) %>% 
  ungroup() %>% 
  dplyr::select(ExperimentalGroup,
                BS_Original,
         Treatment,
         FirstParturitionPairingDayDiff) %>% 
  rename(Delay = FirstParturitionPairingDayDiff) %>% 
  mutate(Delay = as.numeric(Delay)) %>% 
  arrange(Treatment,
          Delay)

```

On average it takes 110 for the first parturition to occurr in the sub treatment and 113 days in the queen treatment.
```{r Summary}

#Raw value 
FirstPart_Data %>% 
  group_by(Treatment) %>% 
  summarize(avg = mean(Delay),
            sd = sd(Delay)) %>% 
  ungroup()
```

I run different types of models for continuous and positive response variables, i.e. gamma, lognormal, inverse gaussian, log transformed response. Don t run tweedie as it causes R to crash

```{r Model}


#Gamma model
DelayFirstPart_Gamma <- glmmTMB(Delay ~ Treatment, 
                             family = Gamma(link = "log"), 
                             data = FirstPart_Data)
summary(DelayFirstPart_Poo)



#Log normal model 
DelayFirstPart_Log <- glmmTMB(Delay ~ Treatment, 
                             family = "lognormal", 
                             data = FirstPart_Data)
summary(DelayFirstPart_Log)

#Inverse gaussian model
DelayFirstPart_IG <- glm(Delay ~ Treatment,  
                             family = inverse.gaussian(link = "log"),  
                             data = FirstPart_Data)
summary(DelayFirstPart_IG)
confint(DelayFirstPart_IG, level = 0.95)


#Log transformed response
DelayFirstPart_LogTrans <- glmmTMB(log(Delay) ~ Treatment, 
                                   family = gaussian, 
                                   data = FirstPart_Data)
summary(DelayFirstPart_LogTrans)

```

Based on AIC lognormal and inverse gaussian are the best models.
```{r AIC}

AIC(DelayFirstPart_IG,DelayFirstPart_Log,DelayFirstPart_LogTrans,DelayFirstPart_Gamma,FirstPart_Weibull)
#Weibull model has higher AIC but that is because yhey use differnt likelihoods calculation and cannot be compared

```

Running model validation  
```{r Model validation}

windows(width=10, height=8)  # Opens a new window with specified dimensions

#Inverse gaussia does seem to pass 
Sim_IG <- simulateResiduals(DelayFirstPart_IG)
plot(Sim_IG)
#KS is nearly significant

#log normal validation
Sim_Log <- simulateResiduals(DelayFirstPart_Log)
plot(Sim_Log)
#KS has a trend

#Log transformed
Sim_LogResp <- simulateResiduals(DelayFirstPart_LogTrans)
plot(Sim_LogResp)
#KS is nearly significant

#Gamma validation 
Sim_Gamma <- simulateResiduals(DelayFirstPart_Gamma)
plot(Sim_Gamma)
#KS significant

```

None of these models validation are really good. What would be the alternative? An alternative would be to use a weibull model 

```{r Weibull}

# Create a survival object
surv_obj <- Surv(FirstPart_Data$Delay)

# Fit Weibull model
FirstPart_Weibull <- survreg(surv_obj ~ Treatment, 
                            data = FirstPart_Data, 
                            dist = "weibull")
# View model summary
summary(FirstPart_Weibull)

# Get confidence intervals
confint(FirstPart_Weibull)
```

I am struggling with the model validation as I cannot use DHARMa (at least that s what the AI suggested)
```{r Model validation}

# Extract model parameters - only the ones that should be constant
model_params <- tibble(
  scale = FirstPart_Weibull$scale,
  shape = 1/FirstPart_Weibull$scale,
  intercept = coef(FirstPart_Weibull)[1],
  treatment_effect = coef(FirstPart_Weibull)[2]
)

# View constant parameters
print(model_params)
View(model_params)

#Model predictions
model_preds <- tibble(
  Delay = FirstPart_Data$Delay,
  Treatment = FirstPart_Data$Treatment,
  linear_pred = predict(FirstPart_Weibull, type = "linear"),
  response_pred = predict(FirstPart_Weibull, type = "response")
)#
print(model_preds)

#Calculate residuals with both versions of Cox-Snell
#cs and weibull are the same because I have no censoring in my data
residuals_df <- tibble(
  Delay = model_preds$Delay,
  Treatment = model_preds$Treatment,
  # Calculate standardized Weibull residuals
  weibull_residuals = (Delay/model_preds$response_pred)^(model_params$shape[1]),
  # Calculate Cox-Snell residuals - version 1 (using pweibull)
  cox_snell_v1 = -log(1 - pweibull(q = Delay, 
                                  shape = model_params$shape[1],
                                  scale = exp(model_preds$linear_pred))),
  # Calculate Cox-Snell residuals - version 2 (using hazard)
  cox_snell_v2 = (Delay/exp(model_preds$linear_pred))^(model_params$shape[1])
)

# View structure
str(residuals_df)
print(residuals_df)
View(residuals_df)

# Create diagnostic plots
# 1. Q-Q plot of residuals
p1 <- ggplot(residuals_df, aes(sample = weibull_residuals)) +
  stat_qq() +
  stat_qq_line() +
  theme_minimal() +
  labs(title = "Q-Q Plot of Residuals")

# 2. Residuals vs fitted values
p2 <- ggplot(residuals_df, aes(x = model_preds$response_pred, y = weibull_residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  theme_minimal() +
  labs(title = "Residuals vs Fitted", x = "Fitted values", y = "Residuals")

# 3. Residuals vs Treatment
p3 <- ggplot(residuals_df, aes(x = Treatment, y = weibull_residuals)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Residuals by Treatment")

# Arrange plots
library(patchwork)
(p1 + p2) / p3

```

However, it seems that we can create DHARMa residuals and I did this refitting a model with the package flexsurv (the same probably could have been done with the package survival).

The vlidation of the weibull modell seems really bad, leaving me with the option to go for byesian approach or a non-parametric bootstrap.

```{r Weibull with flexsurv}

# Load required packages
library(flexsurv)
library(DHARMa)

# Fit Weibull model using flexsurv
FirstPart_Weibull_flex <- flexsurvreg(Surv(Delay) ~ Treatment, 
                                     data = FirstPart_Data, 
                                     dist = "weibull")

# Get parameter estimates
# Extract and explain parameters
params <- coef(FirstPart_Weibull_flex)
#The scale parameter for the sub treatment (baseline) represents the characteristic life (63.2th percentile) of the Weibull distribution for the sub treatment population, measured in days until first parturition.

#The relative scale for queen treatment represents the multiplicative effect on the scale parameter when treatment is Queen. Values greater than 1 indicate longer delays for queens compared to subs.

#The shape parameter indicates how the hazard changes over time. A shape parameter greater than 1 means increasing hazard, while less than 1 means decreasing hazard.


# Get confidence intervals for parameters
confint(FirstPart_Weibull_flex)

# Print full model summary including shape parameter
print(FirstPart_Weibull_flex)

# Ensure the number of simulations matches the number of observations
num_observations <- length(FirstPart_Data$Delay)

# Simulate responses
simulated_time <- simulate(FirstPart_Weibull_flex, nsim = 1000) %>%
select(contains("time"))%>%
#as matrix
as.matrix()

#remove column names
colnames(simulated_time) <- NULL


# Ensure that the newdata argument is specified correctly for predictions
predicted_responses <- predict(FirstPart_Weibull_flex, newdata = FirstPart_Data, type = "response")
View(predicted_responses)

print(dim(simulated_time))
print(length(FirstPart_Data$Delay))
print(nrow(predicted_responses))

# Create DHARMa residuals
sim_residuals <- createDHARMa(
  simulatedResponse = simulated_time,
  observedResponse = FirstPart_Data$Delay,
  fittedPredictedResponse = as.numeric(sim_residuals$fittedPredictedResponse$.pred_time)  # Convert to numeric vector
)

# Check the structure of sim_residuals
str(sim_residuals)

# Attempt to plot again
plot(sim_residuals)

```

Let s try a bayesian approach with brms. What I am not sure about is that many of the posteriors are below 85 days, which is biologically impossible. The issue also exists for non-bayesian models 

```{r Bayesian}

# Ensure Treatment is a factor with "Sub" as the reference level
FirstPart_Data$Treatment <- factor(FirstPart_Data$Treatment, levels = c("Sub", "Queen"))

# Fit a Bayesian log-normal model with correct prior specifications
FirstPart_Bayes_LogNormal_WithPrior <- brm(
  bf(Delay ~ Treatment, sigma ~ 1),  # Define separate formulas for mu and sigma
  family = lognormal(),               # Use log-normal distribution for the response variable
  data = FirstPart_Data,              # Specify the dataset to be used
  prior = c(                          # Specify the priors for the model parameters
    set_prior("normal(100, 10)", class = "Intercept", dpar = "mu"),                     # Prior for mu.Intercept
    set_prior("normal(0, 5)", class = "b", coef = "TreatmentQueen", dpar = "mu"),      # Prior for mu.TreatmentQueen
    set_prior("normal(0, 5)", class = "Intercept", dpar = "sigma")                     # Prior for sigma.Intercept
  ),
  chains = 4,                         # Number of Markov Chains to run
  cores = 4,                          # Number of CPU cores to use for parallel processing
  iter = 2000,                        # Total number of iterations per chain
  warmup = 1000,                      # Number of warmup (burn-in) iterations to discard
  control = list(adapt_delta = 0.99) # Control parameter to improve convergence
)

# Check model summary
summary(FirstPart_Bayes_LogNormal_WithPrior)


```

the validation of the lognormal model is ongoing and not finished
```{r brms lognormal validation}

# Step 1: Convergence Diagnostics

#Assess mixing of chains
 summary(FirstPart_Bayes_LogNormal_WithPrior)
 #Rhat is 1 so indicate good convergence

#Examine effective sample size
summary(FirstPart_Bayes_LogNormal_WithPrior)$fixed
# Bulk and Tail ESS are > 100 indicating sufficient sampling of posterior distribution

# Trace plots for all parameters
plot(FirstPart_Bayes_LogNormal_WithPrior)
#Looks good to me 


#Step 2: Posterior summaries and diagnostics

# Summarize posterior distributions
summary(FirstPart_Bayes_LogNormal_WithPrior)

# Plot posterior distributions
plot(FirstPart_Bayes_LogNormal_WithPrior)


#Step 3: Posterior predictive checks
pp_check(FirstPart_Bayes_LogNormal_WithPrior, ndraws = 1000)
#not really experienced in this but looks ok? ask JT
# Density overlay
pp_check(FirstPart_Bayes_LogNormal_WithPrior, type = "dens_overlay") +
  labs(title = "Posterior Predictive Check: Density Overlay")
# Scatter plot of observed vs. predicted
pp_check(FirstPart_Bayes_LogNormal_WithPrior, type = "scatter_avg") +
  labs(title = "Posterior Predictive Check: Scatter Plot of Observed vs. Predicted")


#Step 4: Residual diagnostic using poestrior predictive checks

# Extract posterior predictions
post_pred <- posterior_predict(FirstPart_Bayes_LogNormal_WithPrior)
  
# Calculate the mean prediction for each observation
# The mean of the 4000 predictions for each observation
mean_pred <- apply(post_pred, 2, mean)
# Calculate residuals (observed - fitted)
residuals <- FirstPart_Data$Delay - mean_pred


# Residuals vs. Fitted Values
ggplot(data = NULL, aes(x = mean_pred, y = residuals)) +
    geom_point(alpha = 0.6, color = "blue") +
    geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
    labs(title = "Residuals vs. Fitted Delays",
         x = "Fitted Delays",
         y = "Residuals") +
    theme_minimal()


#Histogram of residuals
ggplot(data = residuals_df, aes(x = residuals)) +
  geom_histogram(binwidth = 5, fill = "lightblue", color = "black", alpha = 0.7) +
  labs(title = "Histogram of Residuals",
       x = "Residuals",
       y = "Frequency") +
  theme_minimal()


#QQ plot of residuals 
ggplot(data = residuals_df, aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line(color = "red") +
  labs(title = "Q-Q Plot of Residuals") +
  theme_minimal()
#the qqplot is not good at all


#Step 6 
#Outliers and influential data points 
# Calculate standardized residuals
residuals_standardized <- residuals(FirstPart_Bayes_LogNormal_WithPrior, summary = FALSE)$Estimate / 
                           residuals(FirstPart_Bayes_LogNormal_WithPrior, summary = FALSE)$Est.Error

# Add to the dataframe
residuals_df <- residuals_df %>%
  mutate(standardized_residuals = residuals_standardized)

# Plot standardized residuals
ggplot(residuals_df, aes(x = mean_pred, y = standardized_residuals)) +
  geom_point(alpha = 0.6, color = "purple") +
  geom_hline(yintercept = c(-2, 2), color = "red", linetype = "dashed") +
  labs(title = "Standardized Residuals vs. Fitted Values",
       x = "Fitted Values",
       y = "Standardized Residuals") +
  theme_minimal()

```

```{r Validation Performance}

# Calculate Bayesian R-squared
bayes_R2 <- r2(FirstPart_Bayes_LogNormal_WithPrior)
print(bayes_R2)

# Check model performance
model_performance(FirstPart_Bayes_LogNormal_WithPrior)

# Comprehensive model check
poo <- check_model(FirstPart_Bayes_LogNormal_WithPrior)

# Check the class of your model
class(FirstPart_Bayes_LogNormal_WithPrior)

```

```{r}


# Assuming you have a brms model named 'FirstPart_Bayes_LogNormal_WithPrior'

# Calculate and visualize credible intervals
ci <- ci(FirstPart_Bayes_LogNormal_WithPrior)
print(ci)

# Calculate Bayesian R-squared
bayes_R2 <- bayes_R2(FirstPart_Bayes_LogNormal_WithPrior)
print(bayes_R2)

# Compute the Probability of Direction
pd <- p_direction(FirstPart_Bayes_LogNormal_WithPrior)
print(pd)


```

```{r bayesplot}

# Assuming you have a brms model named 'FirstPart_Bayes_LogNormal_WithPrior'

# Trace plots
mcmc_trace(as.array(FirstPart_Bayes_LogNormal_WithPrior), pars = c("b_Intercept", "b_TreatmentQueen"))

# Autocorrelation plots
mcmc_acf(as.array(FirstPart_Bayes_LogNormal_WithPrior), pars = c("b_Intercept", "b_TreatmentQueen"))

# Posterior predictive checks
ppc_dens_overlay(y = FirstPart_Data$Delay, yrep = posterior_predict(FirstPart_Bayes_LogNormal_WithPrior, draws = 100))




```

## Effect of BS on probability to produce the first litter

They produced the first litter in 75% of the case (9/12) but this probability is not significantly different than expected by chance. I guess the correct way to run the model to avoid duplicating the data, is to fix the BF as the focal (or the sister)

TO DO;CHANGE ALL NAMES OF MODELS SO IT IS MORE APPROPRIATE

```{r Data}
#All females from queen treatment    

FirstPart_QT <- DC_Info_F %>%   
  filter(Treatment == "Queen") %>%   
  group_by(ExperimentalGroup) %>%   
  mutate(BredFirst = rank(FirstDCParturitionDate, na.last = "keep", ties.method = "first")) %>%   
  mutate(BredFirst = case_when(BredFirst == 1 ~ "Yes",                                 TRUE ~ "No")) %>%    
  ungroup() %>%    
  dplyr::select(ExperimentalGroup,          
                AnimalID,          
                BS_Original,          
                FirstDCParturitionDate,          
                EvictionStatus,          
                BredFirst)   
#The three cases where the queen did not breed first (and actually did not breed at all) were cases where they were evicted rapidly and di not have time to produce the first litter FirstPart_QT_Q <- FirstPart_QT %>%    filter(BS_Original == "Breeder")     
```

```{r Model}
pBreedFirst_Breeder <- glmmTMB(as.factor(BredFirst) ~ 1,                       family = "binomial",
                               data = FirstPart_QT %>%   
                                 filter(BS_Original == "Breeder")) 
summary(pBreedFirst_Breeder) 
inv.logit(1.0986) 

#Confidence intervals
confint(pBreedFirst_Breeder, level = 0.95)

#taking the daughter perspective is exactly the opposite
pBreedFirst_Daughter <- glmmTMB(as.factor(BredFirst) ~ 1,                       family = "binomial",
                               data = FirstPart_QT %>%   
                                 filter(BS_Original == "Helper")) 
summary(pBreedFirst_Daughter) 

```

```{r Model Validation}
SimR_pBreedFirst <- simulateResiduals( pBreedFirst_Breeder)  
plot(SimR_pBreedFirst) 
```

## Effect of female type on delay til first litter

```{r Data}

FirstPart <- DC_Info_F %>%   
  group_by(ExperimentalGroup) %>%   
  mutate(BredFirst = rank(FirstDCParturitionDate, na.last = "keep", ties.method = "first")) %>%   
  mutate(BredFirst = case_when(BredFirst == 1 ~ "Yes",                                 TRUE ~ "No")) %>%    
  ungroup() %>%    
  dplyr::select(ExperimentalGroup, 
                Treatment,
                AnimalID,          
                BS_Original,          
                FirstDCParturitionDate,   
                PairingDate,
                EvictionStatus,          
                BredFirst) %>% 
  filter(BredFirst == "Yes") %>% 
  mutate(Delay = as.integer(FirstDCParturitionDate - PairingDate))%>% 
  mutate(FemaleType = case_when(Treatment == "Queen" & BS_Original == "Breeder" ~ "Breeder",
                                Treatment == "Queen" & BS_Original == "Helper" ~ "Daughter",
                                Treatment == "Sub" ~ "Sister")) %>% 
  arrange(Treatment,
          FemaleType,
          Delay)

write.csv(FirstPart, "FirstPart.csv",row.names = FALSE)
```

```{r Model}

#Gamma model
FirstPart_LM <- glmmTMB(Delay ~ FemaleType, 
                             data = FirstPart)
summary(FirstPart_Gamma)


#Gamma model
FirstPart_Gamma <- glmmTMB(Delay ~ FemaleType, 
                             family = Gamma(link = "log"), 
                             dispformula = ~ Treatment,
                             data = FirstPart %>% 
                           filter(FemaleType != "Daughter"))
summary(FirstPart_Gamma)



#Log normal model 
FirstPart_Log <- glmmTMB(Delay ~ FemaleType, 
                         family = "lognormal", 
                         dispformula = ~ Treatment,
                         data = FirstPart %>% 
                           filter(FemaleType != "Daughter"))
summary(FirstPart_Log)



#Inverse gaussian model
FirstPart_IG <- glm(Delay ~ FemaleType,  
                    family = inverse.gaussian(link = "log"), 
                    data = FirstPart %>% 
                      filter(FemaleType != "Daughter")
                      )
summary(FirstPart_IG)
View(FirstPart)


#tweedie
FirstPart_Tweedie <- glmmTMB(Delay ~ FemaleType,  
                             family = tweedie(link = "log"), 
                              dispformula = ~ Treatment,
                             data = FirstPart %>% 
                           filter(FemaleType != "Daughter"))
summary(FirstPart_Tweedie)
#Convergence issue 


#Tweedie with mgcv
library(mgcv)
FirstPart_Tweediemgcv <- gam(Delay ~ FemaleType, 
                         family = tw(),
                         data = FirstPart)
summary(FirstPart_Tweediemgcv)

#AIC of gamma model seems to be better 
AIC(FirstPart_IG,FirstPart_Gamma,FirstPart_Log,FirstPart_Tweedie )

FirstPart_LM <- lm(Delay ~ FemaleType, data = FirstPart)
summary(FirstPart_LM)


```

None of the models used produce good model estimates

```{r Model validation}


Sim_FirstPart_Lm <- simulateResiduals(FirstPart_LM )
plot(Sim_FirstPart_Lm)
#KS significant

Sim_FirstPart_Log <- simulateResiduals(FirstPart_Log)
plot(Sim_FirstPart_Log)
#Deviance residuals


Sim_FirstPart_IG <- simulateResiduals(FirstPart_IG)
plot(Sim_FirstPart_IG)
#Deviance residuals


Sim_FirstPart_Gamma <- simulateResiduals(FirstPart_Gamma)
plot(Sim_FirstPart_Gamma)
#Basically all sorst of error


Sim_FirstPart_mgcv <- simulateResiduals(FirstPart_Tweediemgcv)
plot(Sim_FirstPart_Gamma)
#All sorts of errors

```

The alternatives are

-   weibull model

-   brms

-   parametric bootstrap

```{r Weibull}

FirstPart$FemaleType <- as.factor(FirstPart$FemaleType)

# Load necessary libraries
library(survival)

# Fit the Weibull model using survreg
FirstPart_Weibull <- survreg(Surv(Delay) ~ FemaleType, data = FirstPart, dist = "weibull")

# Check the summary of the fitted model
summary(FirstPart_Weibull)


# Generate a sequence of delay times
delay_seq <- seq(from = min(FirstPart$Delay), to = max(FirstPart$Delay), length.out = 100)

# Create a new data frame with these delay times and the corresponding FemaleType
new_data <- expand.grid(FemaleType = levels(FirstPart$FemaleType), Delay = delay_seq)


# Get the predicted survival times for each observation in the new dataset
weibull_surv_individuals <- predict(FirstPart_Weibull, newdata = new_data, type = "response")


# Create an empty plot for survival curves
plot(NA, xlim = c(min(delay_seq), max(delay_seq)), 
     ylim = range(weibull_surv_individuals), 
     xlab = "Delay Time", ylab = "Predicted Survival Time", type = "n")

# Loop through each FemaleType and plot the predicted survival curve
for (i in 1:length(levels(FirstPart$FemaleType))) {
  
  # Filter the predictions and delay times for this FemaleType
  weibull_surv_female <- weibull_surv_individuals[(i - 1) * 100 + (1:100)]
  weibull_data <- delay_seq  # Corresponding delay times
  
  # Plot the predicted survival curve for this FemaleType
  lines(weibull_data, weibull_surv_female, col = i, lty = 2)  # Adjust color and line style
}

# Add a legend for the FemaleType categories
legend("topright", legend = levels(FirstPart$FemaleType), col = 1:length(levels(FirstPart$FemaleType)), lty = 2)




```

```{r}

FirstPart_Bayes <- brm(
  formula = Delay ~ FemaleType,             # Model with FemaleType as predictor
  family = Gamma(link = "log"),             # Using Gamma distribution with log link
  data = FirstPart,                         # Your dataset
  prior = c(
    # Informative priors (can adjust these based on your domain knowledge)
    prior(normal(0, 5), class = "b"),       # Prior for the regression coefficients
    prior(normal(0, 5), class = "Intercept") # Prior for the intercept
  ),
  chains = 4,                              # Number of MCMC chains
  cores = 4,                               # Number of CPU cores
  iter = 2000,                             # Number of iterations per chain
  control = list(adapt_delta = 0.99)        # Control parameters for better convergence
)

# Check model summary
summary(FirstPart_Bayes)


pp_check(FirstPart_Bayes)
# Obtain posterior predictive values
y_pred <- posterior_predict(FirstPart_Bayes)

# Plot residuals
plot(y_pred - FirstPart$Delay)


```

```{r bootstrapping}


library(dplyr)

# Set seed for reproducibility
set.seed(123)

# Number of bootstrap iterations
n_bootstrap <- 1000

# Prepare the data (assumed to be in a tibble or data.frame format)
# Example: Replace with your actual data
# data <- FirstPart

# Step 1: Create a function for calculating the difference in means between groups
bootstrap_diff_in_means <- function(data, group1, group2) {
  mean_group1 <- mean(data$Delay[data$FemaleType == group1], na.rm = TRUE)
  mean_group2 <- mean(data$Delay[data$FemaleType == group2], na.rm = TRUE)
  return(mean_group1 - mean_group2)
}

# Step 2: Set up the bootstrapping process
bootstrap_results <- replicate(n_bootstrap, {
  # Resample with replacement for each group
  resampled_data <- data %>%
    group_by(FemaleType) %>%
    sample_n(size = n(), replace = TRUE)
  
  # Calculate differences in means for each pair of groups
  diff_sister_daughter <- bootstrap_diff_in_means(resampled_data, "Sister", "Daughter")
  diff_sister_mother <- bootstrap_diff_in_means(resampled_data, "Sister", "Mother")
  diff_daughter_mother <- bootstrap_diff_in_means(resampled_data, "Daughter", "Mother")
  
  # Return the differences in means
  c(diff_sister_daughter, diff_sister_mother, diff_daughter_mother)
})

# Step 3: Calculate the observed differences in means from the original data
observed_diff_sister_daughter <- bootstrap_diff_in_means(data, "Sister", "Daughter")
observed_diff_sister_mother <- bootstrap_diff_in_means(data, "Sister", "Mother")
observed_diff_daughter_mother <- bootstrap_diff_in_means(data, "Daughter", "Mother")

# Step 4: Calculate 95% confidence intervals for each difference
conf_int_sister_daughter <- quantile(bootstrap_results[1, ], c(0.025, 0.975))
conf_int_sister_mother <- quantile(bootstrap_results[2, ], c(0.025, 0.975))
conf_int_daughter_mother <- quantile(bootstrap_results[3, ], c(0.025, 0.975))

# Step 5: Calculate p-values by comparing observed values to the bootstrap distribution
p_value_sister_daughter <- mean(abs(bootstrap_results[1, ]) >= abs(observed_diff_sister_daughter))
p_value_sister_mother <- mean(abs(bootstrap_results[2, ]) >= abs(observed_diff_sister_mother))
p_value_daughter_mother <- mean(abs(bootstrap_results[3, ]) >= abs(observed_diff_daughter_mother))

# Print results
cat("Observed Difference (Sister vs Daughter): ", observed_diff_sister_daughter, "\n")
cat("Bootstrap 95% CI (Sister vs Daughter): ", conf_int_sister_daughter, "\n")
cat("P-value (Sister vs Daughter): ", p_value_sister_daughter, "\n")

cat("Observed Difference (Sister vs Mother): ", observed_diff_sister_mother, "\n")
cat("Bootstrap 95% CI (Sister vs Mother): ", conf_int_sister_mother, "\n")
cat("P-value (Sister vs Mother): ", p_value_sister_mother, "\n")

cat("Observed Difference (Daughter vs Mother): ", observed_diff_daughter_mother, "\n")
cat("Bootstrap 95% CI (Daughter vs Mother): ", conf_int_daughter_mother, "\n")
cat("P-value (Daughter vs Mother): ", p_value_daughter_mother, "\n")

```

## Effect of treatment and breeding opportunity on occurrence of eviction

due to complete separation of data, we will have to use a Firth logistic regression

```{r Data}

#Occurrences of eviction 
EvictionOcc <- DC_Info_F %>% 
  distinct(ExperimentalGroup, .keep_all = TRUE) %>% 
  mutate(Eviction = case_when(!is.na(EvictionDate) ~ "Yes",
                              TRUE ~ "No")) %>% 
  mutate(Period = "Post") %>% 
  select(ExperimentalGroup,
         Treatment,
         Period,
         Eviction) %>% 
  bind_rows(., 
            DC_Info_F %>% 
  distinct(ExperimentalGroup,Treatment) %>% 
  mutate(Period = "Pre",
         Eviction = "No")) %>% 
  #RELEVEL
  mutate(Period = fct_relevel(Period,"Pre","Post"),
         Treatment = fct_relevel(Treatment, "Sub", "Queen"))

View(EvictionOcc)


```

```{r Model}

EvictionFirth <- logistf(as.factor(Eviction)~ Period * Treatment, data = EvictionOcc)
summary(EvictionFirth)


```

## Effect of treatment on eviction hazard

Determine the effect of treatment and male introduction on aggressive suppression of subordinate reproduction. Tusker (queen) had a very very late eviction. Lion (Sub) had no eviction til euthanized and DP (Sub) had no eviction til the death of the breeding female.

```{r Data}

#Isolation Info
SA_IsolationInfo <- DC_Info_F %>% 
  distinct(ExperimentalGroup,
           IsolationPeriod,.keep_all = TRUE) %>% 
  mutate(StartTime = 0,
         StopTime = as.numeric(IsolationPeriod),
         Event = 0,
         Male = 0) %>% 
  select(ExperimentalGroup,
         Treatment,
         StartTime,
         StopTime,
         Male,
         Event)


#Eviction Info
SA_PairingInfo <- DC_Info_F %>% 
  filter(!is.na(EvictionDate)) %>% 
  mutate(Event = 1,
         Male = 1) %>% 
  mutate(StartTime = as.numeric(IsolationPeriod)) %>% 
  mutate(StopTime  = as.numeric(IsolationPeriod + EvictionPairingDayDiff)) %>% 
    select(ExperimentalGroup,
           Treatment,
         StartTime,
         StopTime,
         Male,
         Event) %>% 
  distinct()


#Lion 
#Sub Treatment
SA_Lion <- DC_Info_F %>% 
  filter(ExperimentalGroup == "Lion") %>% 
  mutate(Event = 0,
         Male = 1,
         StartTime = as.numeric(IsolationPeriod),
         StopTime = as.numeric(LastDate-PairingDate)) %>% 
      select(ExperimentalGroup,
             Treatment,
         StartTime,
         StopTime,
         Male,
         Event) %>% 
  distinct()


#DP
#Sub treatment
SA_DP <- DC_Info_F %>% 
  filter(ExperimentalGroup == "DevilsPeak",
         AnimalID == "DVF048") %>% 
  mutate(Event = 0,
         Male = 1,
         StartTime = as.numeric(IsolationPeriod),
         StopTime = as.numeric(LastDate-PairingDate)) %>% 
  select(ExperimentalGroup,
         Treatment,
         StartTime,
         StopTime,
         Male,
         Event) %>% 
  distinct()
str(SA_PairingInfo)


#Merge all data together
SA_Data <- bind_rows(SA_IsolationInfo,
                     SA_PairingInfo,
                     SA_Lion,
                     SA_DP) %>% 
  mutate(StopTime = case_when(StartTime == StopTime ~ StopTime+0.5,
                              TRUE ~ StopTime)) %>% 
  mutate(Delay = StopTime - StartTime)



```

Overall there is no effect of treatment on the aggression hazard.

```{r Model All}

#Cox model after the addition of male
Cox_All <- coxph(Surv(Delay, Event) ~ Treatment, data = SA_Data %>% 
                     filter(Male == 1))
summary(Cox_All)

#Checking the proportional hazards assumption
Cox_All_PHcheck <- cox.zph(Cox_All)

# Summarizing the output of the proportional hazards check
summary(Cox_All_PHcheck)$table


```

If we exclude the cases where helper could not evict because they were evicted, the result is the same in that there is no effect of treatment on the aggression hazard

```{r Model helper evictor}

#Cox model after the addition of male
Cox_Helper <- coxph(Surv(Delay, Event) ~ Treatment, data = SA_Data %>% 
                     filter(Male == 1,
                            ExperimentalGroup!= "Angol",
                            ExperimentalGroup!= "Stella"))
summary(Cox_Helper)

#Checking the proportional hazards assumption
Cox_Helper_PHcheck <- cox.zph(Cox_Helper)


```

## Effect of treatment on the delay until aggression

```{r Data}

Delay_Data <- DC_Info_F %>% 
  select(ExperimentalGroup,
         Treatment,
         EvictionPairingDayDiff) %>% 
  filter(!is.na(EvictionPairingDayDiff)) %>% 
  distinct() %>%
  mutate(EvictionPairingDayDiff = as.numeric(EvictionPairingDayDiff)) %>% 
  mutate(EvictionPairingDayDiff = case_when(EvictionPairingDayDiff == 0 ~ 0.5,
         TRUE ~ EvictionPairingDayDiff))

```

```{r Model}

#Gamma model
Delay_Gamma <- glmmTMB(EvictionPairingDayDiff ~ Treatment, 
                             family = Gamma(link = "log"), 
                             data = Delay_Data %>% 
                         filter(ExperimentalGroup!= "Angol",
                                ExperimentalGroup!= "Stella"))
summary(Delay_Gamma)


#Gamma validation 
Sim_Gamma <- simulateResiduals(Delay_Gamma)
plot(Sim_Gamma)



#Log normal model 
Delay_Log <- glmmTMB(EvictionPairingDayDiff ~ Treatment, 
                       family = "lognormal", 
                       data = Delay_Data %>% 
                       filter(ExperimentalGroup!= "Angol",
                              ExperimentalGroup!= "Stella"))
summary(Delay_Log)

#log normal validation
Sim_Log <- simulateResiduals(Delay_Log)
plot(Sim_Log )
# Issue with homogeneity of variance 

#AIC of gamma model seems to be better 
AIC(Delay_Gamma,Delay_Log)


# 
# #Delay Poisson 
# Delay_poisson <- glmmTMB(EvictionPairingDayDiff ~ Treatment, 
#                        family = "poisson", 
#                        data = Delay_Data %>% 
#                        filter(ExperimentalGroup!= "Angol",
#                               ExperimentalGroup!= "Stella"))
# summary(Delay_poisson)
# 
# 
# 
# #Delay negative.binomial 
# #0.5 is an issue 
# Delay_nega <- glmmTMB(EvictionPairingDayDiff ~ Treatment, 
#                        family = "nbinom2", 
#                        data = Delay_Data %>% 
#                        filter(ExperimentalGroup!= "Angol",
#                               ExperimentalGroup!= "Stella"))
# 

AIC(Delay_Gamma,Delay_Log,Delay_poisson,Delay_nega)


```

```{r Prediction}


#Treatment to be used in predict function
pdata_delay <- data.frame(Treatment = c("Queen", "Sub"))

#Inverse link
ilink_delay <- family(Delay_Gamma)$linkinv

#Predicted value on the link scale 
Predicted_delay <- cbind(pdata_delay, as.data.frame(predict(Delay_Gamma, newdata = pdata_G, type = "link", se.fit = TRUE))) %>% 
  #ADD RESPONSE SCALE PREDICTED
  mutate(Fitted = ilink_delay(fit),
         Upper = ilink_delay(fit + (1.96 * se.fit)),
         Lower = ilink_delay(fit - (1.96 * se.fit)))
View(Predicted_delay)


